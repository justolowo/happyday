[
["index.html", "Statistics for Geography (GEOG 533) Preface", " Statistics for Geography (GEOG 533) Dr. Qiusheng Wu 2016-11-17 Preface This is the site for sample lab solutions for Statistics for Geography (GEOG 533) Welcome! "],
["about-the-author.html", "About the Author", " About the Author Dr. Qiusheng Wu (http://wetlands.io) is an Assistant Professor in the Department of Geography, Binghamton University, State University of New York "],
["lab-descriptive-statistics.html", "1 Lab - Descriptive Statistics 1.1 Question 1 1.2 Question 2 1.3 Question 3 1.4 Question 4", " 1 Lab - Descriptive Statistics You can download the original R notebook document HERE 1.1 Question 1 The Type variable classifies the type of market the car is aimed at. Find the cheapest (Price) car in each type, and the car with the greatest fuel efficiency (MPG.highway). Find out the Manufacturer and Model. (20 pt.) 1.1.1 Question 1a find out the cheapest car in each type library(MASS) df &lt;- Cars93 price.min &lt;- tapply(df$Price, df$Type, min) ### get the min price based on type price.min ## Compact Large Midsize Small Sporty Van ## 11.1 18.4 13.9 7.4 10.0 16.3 df$price.min &lt;- price.min[df$Type] ### write the minimum price to a new column df$price.diff &lt;- df$Price - df$price.min ### write the price difference to a new column cheapest &lt;- df[df$price.diff == 0,] ### select the rows with price.diff = 0 cheapest[,c(&quot;Manufacturer&quot;,&quot;Model&quot;,&quot;Type&quot;,&quot;Price&quot;)] ## Manufacturer Model Type Price ## 16 Chevrolet Lumina_APV Van 16.3 ## 20 Chrylser Concorde Large 18.4 ## 31 Ford Festiva Small 7.4 ## 46 Hyundai Scoupe Sporty 10.0 ## 47 Hyundai Sonata Midsize 13.9 ## 74 Pontiac Sunbird Compact 11.1 1.1.2 Question 1b find out the greatest fuel efficiency in each type MPG.max &lt;- tapply(df$MPG.highway, df$Type, max) ### get the max MPG based on type MPG.max ## Compact Large Midsize Small Sporty Van ## 36 28 31 50 36 24 df$MPG.max &lt;- MPG.max[df$Type] ### write the max MPG to a new column df$MPG.diff &lt;- df$MPG.highway - df$MPG.max ### write the MPG difference to a new column best &lt;- df[df$MPG.diff==0,] ### select the rows with MPG.diff = 0 best[,c(&quot;Manufacturer&quot;,&quot;Model&quot;,&quot;Type&quot;,&quot;MPG.highway&quot;)] ## Manufacturer Model Type MPG.highway ## 6 Buick Century Midsize 31 ## 7 Buick LeSabre Large 28 ## 12 Chevrolet Cavalier Compact 36 ## 20 Chrylser Concorde Large 28 ## 30 Eagle Vision Large 28 ## 39 Geo Metro Small 50 ## 40 Geo Storm Sporty 36 ## 56 Mazda MPV Van 24 ## 69 Oldsmobile Cutlass_Ciera Midsize 31 ## 71 Oldsmobile Eighty-Eight Large 28 ## 77 Pontiac Bonneville Large 28 1.2 Question 2 Compute the mean Horsepower for each type, and the difference between each cars horsepower and the mean for its type. Based on the difference values, calculate the skewness and kurtosis (10 pt.) power.mean &lt;- tapply(df$Horsepower, df$Type, mean) ### get the mean hoursepower based on group power.mean ## Compact Large Midsize Small Sporty Van ## 131.0000 179.4545 173.0909 91.0000 160.1429 149.4444 df$power.mean &lt;- power.mean[df$Type] ### write the mean hoursepower to a new column df$power.diff &lt;- df$Horsepower - df$power.mean ### write the hoursepower diff to a new column hist(df$power.diff) library(moments) skewness(df$power.diff) ## [1] 1.09488 kurtosis(df$power.diff) ## [1] 5.503432 The skewness is 1.0948801; the kurtosis is 5.5034317 1.3 Question 3 Create two new data frames for USA and non-USA cars. (10 pt.) df.USA &lt;- df[df$Origin == &quot;USA&quot;,] df.nonUSA &lt;- df[df$Origin == &quot;non-USA&quot;,] if(!require(DT)) install.packages(&quot;DT&quot;) ## Loading required package: DT library(DT) datatable(df.USA,options = list(pageLength = 5),caption = &quot;USA&quot;) datatable(df.nonUSA,options = list(pageLength = 5),caption = &quot;non-USA&quot;) 1.4 Question 4 Use write.csv() to save the USA car data to a file. Read it in and check to see that all the factors are correctly set as factors. (10 pt.) # write.table(df.USA,file = &quot;carsUSA.txt&quot;,sep = &quot;,&quot;) ### write to a txt file # USA.txt &lt;- read.table(&quot;carsUSA.txt&quot;,header = TRUE,sep = &quot;,&quot;) ### read the txt file # str(USA.txt) write.csv(df.USA,&quot;output/carsUSA.csv&quot;) ### write to a csv file USA.csv &lt;- read.csv(&quot;output/carsUSA.csv&quot;) ### read the csv file str(USA.csv) ## &#39;data.frame&#39;: 48 obs. of 34 variables: ## $ X : int 6 7 8 9 10 11 12 13 14 15 ... ## $ Manufacturer : Factor w/ 14 levels &quot;Buick&quot;,&quot;Cadillac&quot;,..: 1 1 1 1 2 2 3 3 3 3 ... ## $ Model : Factor w/ 48 levels &quot;Achieva&quot;,&quot;Aerostar&quot;,..: 10 30 36 35 19 37 9 14 5 31 ... ## $ Type : Factor w/ 6 levels &quot;Compact&quot;,&quot;Large&quot;,..: 3 2 2 3 2 3 1 1 5 3 ... ## $ Min.Price : num 14.2 19.9 22.6 26.3 33 37.5 8.5 11.4 13.4 13.4 ... ## $ Price : num 15.7 20.8 23.7 26.3 34.7 40.1 13.4 11.4 15.1 15.9 ... ## $ Max.Price : num 17.3 21.7 24.9 26.3 36.3 42.7 18.3 11.4 16.8 18.4 ... ## $ MPG.city : int 22 19 16 19 16 16 25 25 19 21 ... ## $ MPG.highway : int 31 28 25 27 25 25 36 34 28 29 ... ## $ AirBags : Factor w/ 3 levels &quot;Driver &amp; Passenger&quot;,..: 2 2 2 2 2 1 3 2 1 3 ... ## $ DriveTrain : Factor w/ 3 levels &quot;4WD&quot;,&quot;Front&quot;,..: 2 2 3 2 2 2 2 2 3 2 ... ## $ Cylinders : int 4 6 6 6 8 8 4 4 6 4 ... ## $ EngineSize : num 2.2 3.8 5.7 3.8 4.9 4.6 2.2 2.2 3.4 2.2 ... ## $ Horsepower : int 110 170 180 170 200 295 110 110 160 110 ... ## $ RPM : int 5200 4800 4000 4800 4100 6000 5200 5200 4600 5200 ... ## $ Rev.per.mile : int 2565 1570 1320 1690 1510 1985 2380 2665 1805 2595 ... ## $ Man.trans.avail : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 2 2 2 1 ... ## $ Fuel.tank.capacity: num 16.4 18 23 18.8 18 20 15.2 15.6 15.5 16.5 ... ## $ Passengers : int 6 6 6 5 6 5 5 5 4 6 ... ## $ Length : int 189 200 216 198 206 204 182 184 193 198 ... ## $ Wheelbase : int 105 111 116 108 114 111 101 103 101 108 ... ## $ Width : int 69 74 78 73 73 74 66 68 74 71 ... ## $ Turn.circle : int 41 42 45 41 43 44 38 39 43 40 ... ## $ Rear.seat.room : num 28 30.5 30.5 26.5 35 31 25 26 25 28.5 ... ## $ Luggage.room : int 16 17 21 14 18 14 13 14 13 16 ... ## $ Weight : int 2880 3470 4105 3495 3620 3935 2490 2785 3240 3195 ... ## $ Origin : Factor w/ 1 level &quot;USA&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Make : Factor w/ 48 levels &quot;Buick Century&quot;,..: 1 2 4 3 5 6 10 11 8 13 ... ## $ price.min : num 13.9 18.4 18.4 13.9 18.4 13.9 11.1 11.1 10 13.9 ... ## $ price.diff : num 1.8 2.4 5.3 12.4 16.3 ... ## $ MPG.max : int 31 28 28 31 28 31 36 36 36 31 ... ## $ MPG.diff : int 0 0 -3 -4 -3 -6 0 -2 -8 -2 ... ## $ power.mean : num 173 179 179 173 179 ... ## $ power.diff : num -63.091 -9.455 0.545 -3.091 20.545 ... library(DT) datatable(USA.csv,options = list(pageLength = 5),caption = &quot;USA&quot;) "],
["lab-discrete-probability.html", "2 Lab - Discrete Probability 2.1 Question 1 2.2 Question 2 2.3 Question 3 2.4 Question 4", " 2 Lab - Discrete Probability You can download the original R notebook document HERE 2.1 Question 1 The Cars93 dataset in the MASS package contains data from 93 cars on sale in the USA in 1993. Randomly select 40 cars as the training dataset and the remaining 53 cars as the test dataset. Save both datasets as csv files. Find out the number of USA and non-USA cars in the training dataset. (10 pt.) 2.1.1 Solution 1 library(MASS) df &lt;- Cars93 set.seed(100) nrows &lt;- nrow(df) id &lt;- 1:nrows train.index &lt;- sort(sample(1:nrows,40)) train.index ## [1] 6 8 11 12 15 16 18 22 23 24 27 28 29 31 32 36 39 40 42 43 47 51 52 ## [24] 53 55 58 59 61 62 65 67 71 72 73 74 78 83 86 89 93 test.index &lt;- id[-train.index] test.index ## [1] 1 2 3 4 5 7 9 10 13 14 17 19 20 21 25 26 30 33 34 35 37 38 41 ## [24] 44 45 46 48 49 50 54 56 57 60 63 64 66 68 69 70 75 76 77 79 80 81 82 ## [47] 84 85 87 88 90 91 92 train.df &lt;- df[train.index,] # library(knitr) # kable(train.df) test.df &lt;- df[test.index,] table(train.df$Origin) ## ## USA non-USA ## 23 17 if(!(require(DT))) install.packages(&quot;DT&quot;) library(DT) datatable(train.df,caption = &quot;Training dataset&quot;,options = list(pageLength = 5)) datatable(test.df,caption = &quot;Test dataset&quot;,options = list(pageLength = 5)) 2.1.2 Solution 2 if(!require(dplyr)) install.packages(&quot;dplyr&quot;) library(dplyr) library(MASS) df &lt;- Cars93 df$id &lt;- 1:nrow(df) train.df &lt;- sample_n(df,size = 40) test.df &lt;- anti_join(df,train.df,by=&quot;id&quot;) # Display the data if(!(require(DT))) install.packages(&quot;DT&quot;) library(DT) datatable(train.df,caption = &quot;Training dataset&quot;,options = list(pageLength = 5)) datatable(test.df,caption = &quot;Test dataset&quot;,options = list(pageLength = 5)) 2.2 Question 2 Assume that the probabilities of boy and girl births are 0.515 and 0.485, respectively. Use the sample() function to simulate 10 births and print out the result. How many boys and girls do you get out of 10 births? Then simulate 10,000 births. How many boys and girls do you get out of 10,000 births? (10 pt.) births &lt;- sample(c(&quot;boy&quot;,&quot;girl&quot;),size = 10,replace = TRUE,prob = c(0.515,0.485)) births ## [1] &quot;boy&quot; &quot;girl&quot; &quot;girl&quot; &quot;girl&quot; &quot;boy&quot; &quot;girl&quot; &quot;girl&quot; &quot;boy&quot; &quot;boy&quot; &quot;girl&quot; table(births) ## births ## boy girl ## 4 6 births &lt;- sample(c(&quot;boy&quot;,&quot;girl&quot;),size = 10000,replace = TRUE,prob = c(0.515,0.485)) table(births) ## births ## boy girl ## 5188 4812 2.3 Question 3 Assume that the probabilities of boy and girl births are 0.515 and 0.485, respectively. What’s the probability of having exactly 3 boys out of 10 births? What’s the probability of having 8 or more boys out of 10 births? Plot both the density function and cumulative probability function of the number of boy births out of 10. (15 pt.) prob1 &lt;- dbinom(3,size = 10,prob = 0.515) prob1 ## [1] 0.1034654 prob2 &lt;- 1 - pbinom(7,size = 10,prob = 0.515) prob2 ## [1] 0.06605068 pbinom(7,size = 10,prob = 0.515,lower.tail = FALSE) ## [1] 0.06605068 n &lt;- 10 x &lt;- 0:n y &lt;- dbinom(x,size = n,prob = 0.515) plot(x,y) plot(x,y,type=&quot;b&quot;,pch=16,col=&quot;black&quot;, xlab=&quot;number of boys in 10 births&quot;,ylab=&quot;probability&quot;) y &lt;- pbinom(x,size = n,prob = 0.515) plot(x,y) plot(x,y,type=&quot;b&quot;,pch=16,col=&quot;black&quot;, xlab=&quot;number of boys in 10 births&quot;,ylab=&quot;probability&quot;) 2.4 Question 4 If cars arrive randomly at a traffic light at the rate of five every ten seconds. What the probability that exactly four cars arrive in the next ten seconds? What’s the probability that more than five cars arrive in the next ten seconds? Plot the density function of 0-20 cars arriving in the next ten seconds. (15 pt.) dpois(4,lambda = 5) ## [1] 0.1754674 1 - ppois(5,lambda = 5) ## [1] 0.3840393 ppois(5,lambda = 5,lower.tail = FALSE) ## [1] 0.3840393 n &lt;- 20 x &lt;- 0:n y &lt;- dpois(x,lambda = 5) plot(x,y) plot(x,y,type = &quot;b&quot;,pch = 16,col=&quot;black&quot;) "],
["lab-continuous-probability.html", "3 Lab - Continuous Probability 3.1 Question 1 3.2 Question 2 3.3 Question 3", " 3 Lab - Continuous Probability You can download the original R notebook document HERE 3.1 Question 1 The number of points scored by each team in a tournament is normally distributed, with mean μ = 32 and standard deviation σ = 7. What is the probability of: 3.1.1 1a A team scoring 20 points or fewer? Plot the density function and shade the area. (5 pt.) pnorm(20,mean = 32,sd = 7) ## [1] 0.04323813 curve(dnorm(x,32,7),from = 10,to = 54) cord.x &lt;- c(10,seq(10,20,0.01),20) cord.y &lt;- c(0,dnorm(seq(10,20,0.01),mean = 32,sd = 7),0) polygon(cord.x,cord.y,density = 10) 3.1.2 1b A team scoring more than 35 points? Plot the density function and shade the area. (5 pt.) pnorm(35,mean = 32,sd = 7,lower.tail = FALSE) ## [1] 0.3341176 curve(dnorm(x,32,7),from = 10,to = 54) cord.x &lt;- c(35,seq(35,54,0.01),54) cord.y &lt;- c(0,dnorm(seq(35,54,0.01),mean = 32,sd = 7),0) polygon(cord.x,cord.y,density = 10) 3.1.3 1c A team scoring between 20 and 40 points? Plot the density function and shade the area. (5 pt.) pnorm(40,mean = 32,sd = 7) - pnorm(20,mean = 32,sd = 7) ## [1] 0.8302129 diff(pnorm(c(20,40),mean = 32,sd = 7)) ## [1] 0.8302129 curve(dnorm(x,32,7),from = 10,to = 54) cord.x &lt;- c(20,seq(20,40,0.01),40) cord.y &lt;- c(0,dnorm(seq(20,40,0.01),mean = 32,sd = 7),0) polygon(cord.x,cord.y,density = 10) 3.2 Question 2 The number of comments per post on a social media site is exponentially distributed, with the average post receiving ten comments. What percentage of posts get: 3.2.1 2a Fewer than three comments? Plot the density function and shade the area. (5 pt.) pexp(3,rate = 1/10) ## [1] 0.2591818 curve(dexp(x,rate = 1/10),from = 0,to = 100) cord.x &lt;- c(0,seq(0,3,0.01),3) cord.y &lt;- c(0,dexp(seq(0,3,0.01),rate = 1/10),0) polygon(cord.x,cord.y,density = 10) 3.2.2 2b More than 20 comments? Plot the density function and shade the area. (5 pt.) pexp(20,rate = 1/10,lower.tail = FALSE) ## [1] 0.1353353 curve(dexp(x,rate = 1/10),from = 0,to = 100) cord.x &lt;- c(20,seq(20,100,0.01),100) cord.y &lt;- c(0,dexp(seq(20,100,0.01),rate = 1/10),0) polygon(cord.x,cord.y,density = 10) 3.2.3 2c Between five and ten comments? Plot the density function and shade the area. (5 pt.) diff(pexp(c(5,10),rate = 1/10)) ## [1] 0.2386512 curve(dexp(x,rate = 1/10),from = 0,to = 100) cord.x &lt;- c(5,seq(5,10,0.01),10) cord.y &lt;- c(0,dexp(seq(5,10,0.01),rate = 1/10),0) polygon(cord.x,cord.y,density = 10) 3.3 Question 3 Basic raster creation and calculation: 3.3.1 3a Create a first raster (30 rows × 30 columns) and assign values to the raster based on random values from a uniform distribution. Plot the raster and the histogram. (5 pt.) library(raster) ras1 &lt;- raster(nrows=30,ncols=30,xmn=0,xmx=30,ymn=0,ymx=30) ras1[] = runif(ncell(ras1)) plot(ras1) hist(ras1) 3.3.2 3b Create a second raster (30 rows × 30 columns) and assign values to the raster based on random values from a normal distribution. Plot the raster and the histogram. (5 pt.) ras2 &lt;- raster(nrows=30,ncols=30,xmn=0,xmx=30,ymn=0,ymx=30) ras2[] = rnorm(ncell(ras2)) plot(ras2) hist(ras2) 3.3.3 3c Create a third raster (30 rows × 30 columns) and assign values to the raster based on cell-by-cell addition of the first raster and the second raster. Plot the raster and the histogram. (5 pt.) ras3 &lt;- ras1 + ras2 plot(ras3) hist(ras3) 3.3.4 3d Calculate the mean value of the third raster. Reclassify the third raster into a binary image: 1 (cell value &gt; mean value) and 0 (cell value &lt;= mean value). Save the reclassified image to your working directory as a TIFF image. (5 pt.) mean(ras3) ## class : RasterLayer ## dimensions : 30, 30, 900 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : 0, 30, 0, 30 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : layer ## values : -2.690102, 3.762328 (min, max) m &lt;- cellStats(ras3,mean) ras3[ras3 &lt; m] &lt;- 0 ras3[ras3 &gt;= m] &lt;- 1 plot(ras3) hist(ras3) writeRaster(ras3,filename = &quot;output/test.tif&quot;,overwrite=TRUE) r &lt;- raster(&quot;output/test.tif&quot;) plot(r) hist(r) "],
["lab-inferential-statistics.html", "4 Lab - Inferential Statistics 4.1 Question 1 4.2 Question 2 4.3 Question 3 4.4 Question 4 4.5 Question 5 4.6 Question 6 4.7 Question 7 4.8 Question 8 4.9 Question 9 4.10 Question 10", " 4 Lab - Inferential Statistics Complete the following exercises in Chapter 5 (Inferential Statistics) of the textbook [R] pages 172-176. 4.1 Question 1 This is Exercise 1 of the textbook. A planner wishes to estimate average household size for a community within 0.2. The planner desires a 95% confidence level. A small survey indicates that the standard deviation of household size is 2.0. How large should the sample be? q95 &lt;- qnorm(0.975) # std.err = q95 * std /(sqrt(n)) = 0.2 n &lt;- (q95 * 2 / 0.2)^2 n &lt;- ceiling(n) # round up the number n ## [1] 385 #validate the result conf.95 &lt;- q95 * (2 / sqrt(n)) conf.95 ## [1] 0.199778 The minimum sample size should be 385. 4.2 Question 2 This is Exercise 3 of the textbook. The tolerable level of a certain pollutant is 16 mg/l. A researcher takes a sample of size n = 50, and finds that the mean level of the pollutant is 18.5 mg/l, with a standard deviation of 7 mg/l. Construct a 95% confidence interval around the sample mean, and determine whether the tolerable level is within this interval. 4.2.1 Solution 1 n &lt;- 50 m &lt;- 18.5 std &lt;- 7 q95 &lt;- qnorm(0.975) std.err &lt;- std / sqrt(n) conf.95 &lt;- c(m - q95*std.err,m + q95*std.err) conf.95 ## [1] 16.55973 20.44027 x &lt;- 16 if(x &gt;= conf.95[1] &amp; x &lt;= conf.95[2]){ print(&quot;The tolerable level is within the 95% confidence interval.&quot;) } else { print(&quot;The tolerable level is outside the 95% confidence interval.&quot;) } ## [1] &quot;The tolerable level is outside the 95% confidence interval.&quot; 4.2.2 Solution 2 library(MASS) x &lt;- mvrnorm(n = 50,mu = 18.5,Sigma = 7*7,empirical = TRUE) result &lt;- t.test(x,mu = 16) result ## ## One Sample t-test ## ## data: x ## t = 2.5254, df = 49, p-value = 0.01484 ## alternative hypothesis: true mean is not equal to 16 ## 95 percent confidence interval: ## 16.51062 20.48938 ## sample estimates: ## mean of x ## 18.5 if(result$p.value &gt; 0.05){ print(&quot;The tolerable level is within the 95% confidence interval.&quot;) } else { print(&quot;The tolerable level is outside the 95% confidence interval.&quot;) } ## [1] &quot;The tolerable level is outside the 95% confidence interval.&quot; 4.3 Question 3 This is Exercise 5 of the textbook. The proportion of people changing residence in the USA each year is 0.165. A researcher believes that the proportion may be different in the town of Amherst. She surveys 50 individuals in the town of Amherst and finds that the proportion who moved last year is 0.24. Is there evidence to conclude that the town has a mobility rate that is different from the national average? Use α = 0.05 and find a 90% confidence interval around the sample proportion, and state your conclusion. result &lt;- prop.test(x=50*0.24,n = 50,p = 0.165,conf.level = 0.9) result ## ## 1-sample proportions test with continuity correction ## ## data: 50 * 0.24 out of 50, null probability 0.165 ## X-squared = 1.5333, df = 1, p-value = 0.2156 ## alternative hypothesis: true p is not equal to 0.165 ## 90 percent confidence interval: ## 0.1475106 0.3617760 ## sample estimates: ## p ## 0.24 if(result$p.value &gt; 0.05){ print(&quot;There is no evidence to conclude that the town has a mobility rate that is different from the national average&quot;) } else { print(&quot;There is evidence to conclude that the town has a mobility rate that is different from the national average&quot;)} ## [1] &quot;There is no evidence to conclude that the town has a mobility rate that is different from the national average&quot; 4.4 Question 4 This is Exercise 7 of the textbook. A survey of the white and nonwhite population in a local area reveals the following annual trip frequencies to the nearest state park: \\(\\bar{x_{1}}=4.1\\), \\(s_{1}^{2} = 14.3\\), \\(n_{1} = 20\\) \\(\\bar{x_{2}}=3.1\\), \\(s_{2}^{2} = 12.0\\), \\(n_{1} = 16\\) where the subscript ‘1’ denotes the white population and the subscript ‘2’ denotes the nonwhite population. Assume that the variances are equal, and test the null hypothesis that there is no difference between the park-going frequencies of whites and nonwhites. Repeat the exercise, assuming that the variances are unequal. Find the p-value associated with the tests in parts (a) and (b). Associated with the test in part (a), find a 95% confidence interval for the difference in means. Repeat parts (a)–(d), assuming sample sizes of n1 = 24 and n2 = 12. ### 4a library(MASS) x1 &lt;- mvrnorm(n = 20,mu = 4.1,Sigma = 14.3,empirical = TRUE) x2 &lt;- mvrnorm(n = 16,mu = 3.1,Sigma = 12.0,empirical = TRUE) result &lt;- t.test(x1,x2,var.equal = TRUE) result ## ## Two Sample t-test ## ## data: x1 and x2 ## t = 0.81797, df = 34, p-value = 0.4191 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.484493 3.484493 ## sample estimates: ## mean of x mean of y ## 4.1 3.1 if(result$p.value &gt; 0.05){ print(&quot;There is no difference between the park-going frequencies of whites and nonwhites&quot;) } else { print(&quot;There is significant difference between the park-going frequencies of whites and nonwhites&quot;)} ## [1] &quot;There is no difference between the park-going frequencies of whites and nonwhites&quot; ### 4b result &lt;- t.test(x1,x2,var.equal = FALSE) result ## ## Welch Two Sample t-test ## ## data: x1 and x2 ## t = 0.82619, df = 33.323, p-value = 0.4146 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.461613 3.461613 ## sample estimates: ## mean of x mean of y ## 4.1 3.1 if(result$p.value &gt; 0.05){ print(&quot;There is no difference between the park-going frequencies of whites and nonwhites&quot;) } else { print(&quot;There is significant difference between the park-going frequencies of whites and nonwhites&quot;)} ## [1] &quot;There is no difference between the park-going frequencies of whites and nonwhites&quot; ## https://www.youtube.com/watch?v=7GXnzQ2CX58 ### 4c t.var.equal &lt;- t.test(x1,x2,var.equal = TRUE) print(t.var.equal$p.value) ## [1] 0.4190693 t.var.unequal &lt;- t.test(x1,x2,var.equal = FALSE) print(t.var.unequal$p.value) ## [1] 0.4145715 ### 4d print(t.var.equal$conf.int) ## [1] -1.484493 3.484493 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 print(t.var.unequal$conf.int) ## [1] -1.461613 3.461613 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## 4e x1 &lt;- mvrnorm(n = 24,mu = 4.1,Sigma = 14.3,empirical = TRUE) x2 &lt;- mvrnorm(n = 12,mu = 3.1,Sigma = 12.0,empirical = TRUE) t.var.equal &lt;- t.test(x1,x2,var.equal = TRUE) print(t.var.equal$p.value) ## [1] 0.4476643 print(t.var.equal$conf.int) ## [1] -1.645421 3.645421 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 t.var.unequal &lt;- t.test(x1,x2,var.equal = FALSE) print(t.var.unequal$p.value) ## [1] 0.4363646 print(t.var.unequal$conf.int) ## [1] -1.607549 3.607549 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 4.5 Question 5 This is Exercise 9 of the textbook. A researcher suspects that the level of a particular stream’s pollutant is higher than the allowable limit of 4.2 mg/l. A sample of n = 17 reveals a mean pollutant level of = 6.4 mg/l, with a standard deviation of 4.4 mg/l. Is there sufficient evidence that the stream’s pollutant level exceeds the allowable limit? What is the p-value? library(MASS) x &lt;- mvrnorm(n = 17,mu = 6.4,Sigma = 4.4*4.4,empirical = TRUE) result &lt;- t.test(x,mu = 4.2) if(result$p.value &gt; 0.05){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We cannot reject the null hypothesis&quot; print(result$p.value) ## [1] 0.05588524 4.6 Question 6 This is Exercise 13 of the textbook. Suppose we want to know whether the mean length of unemployment differs among the residents of two local communities. Sample information is as follows: Community A: sample mean = 3.4 months, s = 1.1 month, n = 52 Community B: sample mean = 2.8 months, s = 0.8 month, n = 62 Set up the null and alternative hypotheses. Use α = 0.05. Choose a particular test, and show the rejection regions on a diagram. Calculate the test statistic, and decide whether to reject the null hypothesis. (Do not assume that the two standard deviations are equal to one another – therefore a pooled estimate of s should not be found.) A &lt;- mvrnorm(n = 52,mu = 3.4,Sigma = 1.1*1.1,empirical = TRUE) B &lt;- mvrnorm(n = 62,mu = 2.8,Sigma = 0.8*0.8,empirical = TRUE) result &lt;- t.test(A,B,var.equal = FALSE) if(result$p.value &gt; 0.05){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We reject the null hypothesis&quot; print(result$p.value) ## [1] 0.001500385 4.7 Question 7 This is Exercise 15 of the textbook. Find the 90% and 95% confidence intervals for the following mean stream link lengths: 100, 426, 322, 466, 112, 155, 388, 1155, 234, 324, 556, 221, 18, 133, 177, 441. x &lt;- c(100,426,322,466,112,155,388,1155,234,324,556,221,18,133,177,441) result &lt;- t.test(x,conf.level = 0.9) print(result$conf.int) ## [1] 208.7856 444.7144 ## attr(,&quot;conf.level&quot;) ## [1] 0.9 result &lt;- t.test(x,conf.level = 0.95) print(result$conf.int) ## [1] 183.3228 470.1772 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 4.8 Question 8 This is Exercise 16 of the textbook. A researcher surveys 50 individuals in Smithville and 40 in Amherst, finding that 30% of Smithville residents moved last year, while only 22% of Amherst residents did. Is there enough evidence to conclude that mobility rates in the two communities differ? Use a two-tailed alternative, and α = 0.10. Again, find the p-value and a 90% confidence interval for the difference in proportions. result &lt;- prop.test(x=c(50*0.3,40*0.22),n = c(50,40),conf.level = 0.9) if(result$p.value &gt; 0.1){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We cannot reject the null hypothesis&quot; print(result$p.value) ## [1] 0.5388245 4.9 Question 9 This is Exercise 17 of the textbook. A survey of two towns is carried out to see whether there are differences in levels of education. Town A has a mean of 12.4 years of education among its residents; Town B has a mean of 14.4 years. Fifteen residents were surveyed in each town. The sample standard deviation was 3.0 in Town A, and 4.0 in Town B. Is there a significant difference in education between the two towns? Assume the variances are equal. Assume the variances are not equal. In each case, state the null and alternative hypotheses, and test the null hypothesis, using α = 0.05. Find the p-values and a 95% confidence interval for the difference. library(MASS) x1 &lt;- mvrnorm(n = 15,mu = 12.4,Sigma = 3*3,empirical = TRUE) x2 &lt;- mvrnorm(n = 15,mu = 14.4,Sigma = 4*4,empirical = TRUE) result &lt;- t.test(x1,x2,var.equal = TRUE) if(result$p.value &gt; 0.05){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We cannot reject the null hypothesis&quot; print(result$p.value) ## [1] 0.1325654 result &lt;- t.test(x1,x2,var.equal = FALSE) if(result$p.value &gt; 0.05){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We cannot reject the null hypothesis&quot; print(result$p.value) ## [1] 0.1334395 4.10 Question 10 This is Exercise 20 of the textbook. A survey of n = 50 people reveals that the proportion of residents in a community who take the bus to work is 0.15. Is this significantly different from the statewide average of 0.10? Use a Type I error probability of 0.05. result &lt;- prop.test(x = 50*0.15,n = 50,p = 0.1) if(result$p.value &gt; 0.05){ print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;)} ## [1] &quot;We cannot reject the null hypothesis&quot; print(result$p.value) ## [1] 0.3457786 "],
["lab-analysis-of-variance.html", "5 Lab - Analysis of Variance 5.1 Question 1 5.2 Question 2 5.3 Question 3 5.4 Question 4 5.5 Question 5 5.6 Question 6", " 5 Lab - Analysis of Variance Complete the following exercises in Chapter 6 (Analysis of Variance) of the textbook pages 199-203. For each question, you need to specify the null hypothesis and why you accept or reject the null hypothesis. 5.1 Question 1 This is Exercise 2 in Chapter 6 of the Textbook [R]. 5.1.1 Problem Assume that an analysis of variance is conducted for a study where there are \\(N = 50\\) observations and \\(k = 5\\) categories. Fill in the blanks in the following ANOVA table: Sum of squares Degrees of freedom Mean square F Between 116.3 Within 2000 Total 5.1.2 Solution n &lt;- 50 k &lt;- 5 WSS &lt;- 2000 m.sq1 &lt;- 116.3 df1 &lt;- k - 1 df2 &lt;- n - k BSS &lt;- df1 * m.sq1 m.sq2 &lt;- WSS/df2 TSS &lt;- WSS + BSS F.ratio &lt;- m.sq1/m.sq2 ## 2.61675 F.critical &lt;- qf(0.95,df1,df2) ## 2.5787 if(F.ratio &gt; F.critical) { print(&quot;We reject the null hypothesis&quot;) } else { print(&quot;we cannot reject the null hypothesis&quot;) } ## [1] &quot;We reject the null hypothesis&quot; Sum of squares Degrees of freedom Mean square F Between 465.2 4 116.3 2.61675 Within 2000 45 44.4444444 Total 2465.2 5.2 Question 2 This is Exercise 6 in Chapter 6 of the Textbook [R]. 5.2.1 Problem Is there a significant difference between the distances moved by low- and high-income individuals? Twelve respondents in each of the income categories are interviewed, with the following results for the distances associated with residential moves: Respondent Low income High income 1 5 25 2 7 24 3 9 8 4 11 2 5 13 11 6 8 10 7 10 10 8 34 66 9 17 113 10 50 1 11 17 3 12 25 5 Mean 17.17 23.17 Std. dev. 13.25 33.45 Test the null hypothesis of homogeneity of variances by forming the ratio \\(s_1^2 / s_2^2\\) which has an F-ratio with \\(n_1 – 1\\) and \\(n_2 – 1\\) degrees of freedom. Then use ANOVA (with \\(\\alpha = 0.10\\)) to test whether there are differences in the two population means. Set up the null and alternative hypotheses, choose a value of α and a test statistic, and test the null hypothesis. What assumption of the test is likely not satisfied? 5.2.2 Solution L &lt;- c(5,7,9,11,13,8,10,34,17,50,17,25) H &lt;- c(25,24,8,2,11,10,10,66,113,1,3,5) m1 &lt;- mean(L) m2 &lt;- mean(H) s1 &lt;- var(L) s2 &lt;- var(H) income &lt;- c(L,H) group &lt;- c(rep(&quot;L&quot;,12),rep(&quot;H&quot;,12)) df &lt;- data.frame(income,group) F &lt;- s1/s2 #0.157 F.ratio &lt;- df(0.9,df1 = 11,df2 = 11) #0.708 library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode leveneTest(income~group,data = df) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 1.0206 0.3234 ## 22 m &lt;- aov(income~group, data = df) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 1 216 216.0 0.334 0.569 ## Residuals 22 14241 647.3 kruskal.test(L,H) ## ## Kruskal-Wallis rank sum test ## ## data: L and H ## Kruskal-Wallis chi-squared = 10.846, df = 10, p-value = 0.3697 shapiro.test(L) ## ## Shapiro-Wilk normality test ## ## data: L ## W = 0.81099, p-value = 0.01253 shapiro.test(H) ## ## Shapiro-Wilk normality test ## ## data: H ## W = 0.67351, p-value = 0.0004738 5.3 Question 3 This is Exercise 9 in Chapter 6 of the Textbook [R]. 5.3.1 Problem A sample is taken of incomes in three neighborhoods, yielding the following data: A B C Overall (Combined sample) N 12 10 8 30 Mean 43.2 34.3 27.2 35.97 Std. dev. 36.2 20.3 21.4 29.2 Use analysis of variance (with α = 0.05) to test the null hypothesis that the means are equal. 5.3.2 Solution library(MASS) A &lt;- mvrnorm(12,mu = 43.2,Sigma = 36.2^2,empirical = TRUE) B &lt;- mvrnorm(10,mu = 34.3,Sigma = 20.3^2,empirical = TRUE) C &lt;- mvrnorm(8,mu = 27.2,Sigma = 21.4^2,empirical = TRUE) k &lt;- 3 n &lt;- 30 values &lt;- c(A,B,C) groups &lt;- c(rep(&quot;A&quot;,12),rep(&quot;B&quot;,10),rep(&quot;C&quot;,8)) df &lt;- data.frame(values,groups) m &lt;- aov(values~groups,data = df) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 2 1270 635.2 0.804 0.458 ## Residuals 27 21329 790.0 p.value &lt;- summary(m)[[1]][[1,&quot;Pr(&gt;F)&quot;]] if(p.value &gt; 0.05) { print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;) } ## [1] &quot;We cannot reject the null hypothesis&quot; TukeyHSD(m) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = values ~ groups, data = df) ## ## $groups ## diff lwr upr p adj ## B-A -8.9 -38.73854 20.93854 0.7424056 ## C-A -16.0 -47.80799 15.80799 0.4365612 ## C-B -7.1 -40.15583 25.95583 0.8561024 plot(TukeyHSD(m)) 5.4 Question 4 This is Exercise 10 in Chapter 6 of the Textbook [R]. 5.4.1 Problem Use the Kruskal–Wallis test (with α = 0.05) to determine whether you should reject the null hypothesis that the means of the four columns of data are equal: Col 1 Col 2 Col 3 Col 4 23.1 43.1 56.5 10002.3 13.3 10.2 32.1 54.4 15.6 16.2 43.3 8.7 1.2 0.2 24.4 54.4 5.4.2 Solution c1 &lt;- c(23.1,13.3,15.6,1.2) c2 &lt;- c(43.1,10.2,16.2,0.2) c3 &lt;- c(56.5,32.1,43.3,24.4) c4 &lt;- c(10002.3,54.4,8.7,54.4) values &lt;- c(c1,c2,c3,c4) groups &lt;- c(rep(&quot;c1&quot;,4),rep(&quot;c2&quot;,4),rep(&quot;c3&quot;,4),rep(&quot;c4&quot;,4)) df &lt;- data.frame(values,groups) result &lt;- kruskal.test(values~groups,data = df) if(result$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;) } ## [1] &quot;We cannot reject the null hypothesis&quot; 5.5 Question 5 This is Exercise 12 in Chapter 6 of the Textbook [R]. 5.5.1 Problem A researcher wishes to know whether distance traveled to work varies by income. Eleven individuals in each of three income groups are surveyed. The resulting data are as follows (in commuting miles, one-way): ## This is the script to generate the table. Do not write your answer inside in this block. Observations &lt;- seq(1:11) Low &lt;- c(5,4,1,2,3,10,6,6,4,12,11) Medium &lt;- c(10,10,8,6,5,3,16,20,7,3,2) High &lt;- c(8,11,15,19,21,7,7,4,3,17,18) df &lt;- data.frame(Observations,Low,Medium,High) library(knitr) kable(df) Observations Low Medium High 1 5 10 8 2 4 10 11 3 1 8 15 4 2 6 19 5 3 5 21 6 10 3 7 7 6 16 7 8 6 20 4 9 4 7 3 10 12 3 17 11 11 2 18 Use analysis of variance (with α = 0.05) to test the hypothesis that commuting distances do not vary by income. Also evaluate (using R and the Levene test) the assumption of homoscedasticity. Finally, lump all of the data together and produce a histogram, and comment on whether the assumption of normality appears to be satisfied. 5.5.2 Solution L &lt;- c(5,4,1,2,3,10,6,6,4,12,11) M &lt;- c(10,10,8,6,5,3,16,20,7,3,2) H &lt;- c(8,11,15,19,21,7,7,4,3,17,18) values &lt;- c(L,M,H) groups &lt;- c(rep(&quot;L&quot;,11),rep(&quot;M&quot;,11),rep(&quot;H&quot;,11)) df &lt;- data.frame(values,groups) m &lt;- aov(values~groups,data = df) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 2 201.0 100.48 3.493 0.0433 * ## Residuals 30 862.9 28.76 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(m) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = values ~ groups, data = df) ## ## $groups ## diff lwr upr p adj ## L-H -6.000000 -11.637743 -0.3622571 0.0351337 ## M-H -3.636364 -9.274107 2.0013792 0.2653924 ## M-L 2.363636 -3.274107 8.0013792 0.5619154 plot(TukeyHSD(m)) p.value &lt;- summary(m)[[1]][[1,&quot;Pr(&gt;F)&quot;]] if(p.value &gt; 0.05) { print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;) } ## [1] &quot;We reject the null hypothesis&quot; library(car) leveneTest(values~groups,data = df) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 2.1694 0.1318 ## 30 ## p-value = 0.131 ## the assumption of homoscedasticity is satisfied. hist(values) shapiro.test(values) ## ## Shapiro-Wilk normality test ## ## data: values ## W = 0.90693, p-value = 0.008099 ## p-value = 0.008 &lt; 0.05 ## the assumption of normality not satisfied 5.6 Question 6 This is Exercise 13 in Chapter 6 of the Textbook [R]. 5.6.1 Problem Data are collected on automobile ownership by surveying residents in central cities, suburbs and rural areas. The results are: Central cities Suburbs Rural areas Number of observations 10 15 15 mean 1.5 2.6 1.2 Std. dev 1.0 1.1 1.2 Overall mean: 1.725 Overall std.dev: 1.2 Test the null hypothesis that the means are equal in all three areas. 5.6.2 Solution city &lt;- mvrnorm(10,mu = 1.5,Sigma = 1,empirical = TRUE) suburb &lt;- mvrnorm(15,mu = 2.6,Sigma = 1.1^2,empirical = TRUE) rural &lt;- mvrnorm(15,mu = 1.2,Sigma = 1.2^2,empirical = TRUE) values &lt;- c(city,suburb,rural) groups &lt;- c(rep(&quot;city&quot;,10),rep(&quot;suburb&quot;,15),rep(&quot;rural&quot;,15)) df &lt;- data.frame(values,groups) m &lt;- aov(values~groups,data = df) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 2 15.9 7.950 6.381 0.00416 ** ## Residuals 37 46.1 1.246 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(m) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = values ~ groups, data = df) ## ## $groups ## diff lwr upr p adj ## rural-city -0.3 -1.41257183 0.8125718 0.7888370 ## suburb-city 1.1 -0.01257183 2.2125718 0.0531846 ## suburb-rural 1.4 0.40488550 2.3951145 0.0041221 plot(TukeyHSD(m)) p.value &lt;- summary(m)[[1]][[1,&quot;Pr(&gt;F)&quot;]] if(p.value &gt; 0.05) { print(&quot;We cannot reject the null hypothesis&quot;) } else { print(&quot;We reject the null hypothesis&quot;) } ## [1] &quot;We reject the null hypothesis&quot; ### p-value = 0.004 &lt; 0.05 ### conclusion: reject the null hypothesis ## the suburbs-rural group are significantly different "],
["lab-correlation.html", "6 Lab - Correlation 6.1 Question 1 6.2 Question 2 6.3 Question 3 6.4 Question 4 6.5 Question 5 6.6 Question 6 6.7 Question 7 6.8 Question 8 6.9 Question 9", " 6 Lab - Correlation 6.1 Question 1 This is Exercise 1 in Chapter 7 of the Textbook [R]. 6.1.1 Problem 1a Find the correlation coefficient, r, for the following sample data on income and education: library(knitr) obs &lt;- seq(1:5) income &lt;- c(30,28,52,40,35) edu &lt;- c(12,13,18,16,17) df &lt;- data.frame(obs,income,edu) names(df) &lt;- c(&quot;Observation&quot;,&quot;Income ($*1000)&quot;,&quot;Education (Years)&quot;) kable(df) Observation Income ($*1000) Education (Years) 1 30 12 2 28 13 3 52 18 4 40 16 5 35 17 Solution 1a income &lt;- c(30,28,52,40,35) edu &lt;- c(12,13,18,16,17) result_1a &lt;- cor(x = edu,y = income,method = &quot;pearson&quot;) result_1a ## [1] 0.83577 The correlation coefficient is 0.83577. 6.1.2 Problem 1b Test the null hypothesis ρ = 0. Solution 1b result_1b &lt;- cor.test(x = edu,y = income,method = &quot;pearson&quot;) if(result_1b$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.1.3 Problem 1c Find Spearman’s rank correlation coefficient for these data. Solution 1c result_1c &lt;- cor(x = edu,y = income,method = &quot;spearman&quot;) result_1c ## [1] 0.8 The Spearman’s rank correlation coefficient is 0.8. 6.1.4 Problem 1d Test whether the observed value of rs from part (c) is significantly different from zero. Solution 1d result_1d &lt;- cor.test(x = edu,y = income,method = &quot;spearman&quot;) result_1d ## ## Spearman&#39;s rank correlation rho ## ## data: edu and income ## S = 4, p-value = 0.1333 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8 if(result_1d$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.2 Question 2 This is Exercise 3 in Chapter 7 of the Textbook [R]. 6.2.1 Problem The distribution of the t-statistic for testing the significance of a correlation coefficient has n – 2 degrees of freedom. If the sample size is 36 and α = 0.05, what is the smallest absolute value a correlation coefficient must have to be significant? What if the sample size is 80? 6.2.2 Solution n=36 n &lt;- 36 r.36 &lt;- round(2/sqrt(n),3) # r &lt;- 0.3333 t.critical &lt;- qt(0.975,df = n - 2) ## = 2.0322 t &lt;- r.36*sqrt(n-2)/sqrt(1-r.36^2) ## = 2.0615 When the sample size is 36, the smallest absolute value a correlation coefficient must have to be significant is 0.333 n=80 n &lt;- 80 r.80 &lt;- 2/sqrt(n) # r &lt;- 0.2236 t.critical &lt;- qt(0.975,df = n - 2) ## = 1.9908 t &lt;- r.80*sqrt(n-2)/sqrt(1-r.80^2) ## = 2.0261 When the sample size is 80, the smallest absolute value a correlation coefficient must have to be significant is 0.2236068 6.3 Question 3 This is Exercise 4 in Chapter 7 of the Textbook [R]. 6.3.1 Problem Find the correlation coefficient for the following data: library(knitr) Obs &lt;- seq(1:4) X &lt;- c(2,8,9,7) Y &lt;- c(6,6,10,4) df &lt;- data.frame(Obs,X,Y) kable(df) Obs X Y 1 2 6 2 8 6 3 9 10 4 7 4 6.3.2 Solution X &lt;- c(2,8,9,7) Y &lt;- c(6,6,10,4) cor(X,Y,method = &quot;pearson&quot;) ## [1] 0.3834129 result_3 &lt;- cor.test(X,Y,method = &quot;pearson&quot;) if(result_3$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.4 Question 4 This is Exercise 6 in Chapter 7 of the Textbook [R]. 6.4.1 Problem Find the correlation coefficient between median annual income in the United States and the number of horse races won by the leading jockey, for the period 1984–1995. Test the hypothesis that the true correlation coefficient is equal to zero. Interpret your results. year &lt;- 1984:1994 income &lt;- c(35165,35778,37027,37256,37512,37997,37343,36054,35593,35241,35486) races &lt;- c(399,469,429,450,474,598,364,430,433,410,317) df &lt;- data.frame(year,income,races) names(df) &lt;- c(&quot;Year&quot;,&quot;Median income&quot;,&quot;Number of races won by leading jockey&quot;) kable(df) Year Median income Number of races won by leading jockey 1984 35165 399 1985 35778 469 1986 37027 429 1987 37256 450 1988 37512 474 1989 37997 598 1990 37343 364 1991 36054 430 1992 35593 433 1993 35241 410 1994 35486 317 6.4.2 Solution income &lt;- c(35165,35778,37027,37256,37512,37997,37343,36054,35593,35241,35486) races &lt;- c(399,469,429,450,474,598,364,430,433,410,317) cor(x = races,y = income) ## [1] 0.5583199 # r = 0.5583199 r.min &lt;- 2/sqrt(11) # r.min = 0.603 result_4 &lt;- cor.test(x = races,y = income) if(result_4$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.5 Question 5 This is Exercise 7 in Chapter 7 of the Textbook [R]. 6.5.1 Problem For the following ranked data, find Spearman’s r, and then test the null hypothesis (using a Type I error probability of 0.10) that the true correlation is equal to zero. library(knitr) obs &lt;- 1:6 x &lt;- c(1,2,5,6,11,12) y &lt;- c(8,4,12,3,10,7) df &lt;- data.frame(obs,x,y) names(df) &lt;- c(&quot;Observation&quot;,&quot;Rank of x&quot;,&quot;Rank of y&quot;) kable(df) Observation Rank of x Rank of y 1 1 8 2 2 4 3 5 12 4 6 3 5 11 10 6 12 7 6.5.2 Solution x &lt;- c(1,2,5,6,11,12) y &lt;- c(8,4,12,3,10,7) cor(x,y,method = &quot;spearman&quot;) ## [1] -0.02857143 # r = -0.02857143 result_5 &lt;- cor.test(x,y,method = &quot;spearman&quot;,conf.level = 0.95) if(result_5$p.value&gt;0.1) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.6 Question 6 This is Exercise 8 in Chapter 7 of the Textbook [R]. 6.6.1 Problem Find Pearson’s r for the following data, and then test the null hypothesis that the correlation coefficient is equal to zero. Use a Type I error probability of 0.05. library(knitr) obs &lt;- 1:6 x &lt;- c(3.2,2.4,1.6,8.3,7.2,5.1) y &lt;- c(6.2,7.3,8.1,2.6,6.3,4.3) df &lt;- data.frame(obs,x,y) names(df) &lt;- c(&quot;Observation&quot;,&quot;x&quot;,&quot;y&quot;) kable(df) Observation x y 1 3.2 6.2 2 2.4 7.3 3 1.6 8.1 4 8.3 2.6 5 7.2 6.3 6 5.1 4.3 6.6.2 Solution x &lt;- c(3.2,2.4,1.6,8.3,7.2,5.1) y &lt;- c(6.2,7.3,8.1,2.6,6.3,4.3) cor(x,y) ## [1] -0.8073491 # r = -0.8073491 result_6 &lt;- cor.test(x,y,conf.level = 0.95) if(result_6$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.7 Question 7 This is Exercise 9 in Chapter 7 of the Textbook [R]. 6.7.1 Problem Using R and the Milwaukee dataset, find the correlation between number of bedrooms and lot size. 6.7.2 Solution library(knitr) url &lt;- &quot;http://spatial.binghamton.edu/geog533/data/Milwaukee_Sales.csv&quot; df &lt;- read.csv(url,header = TRUE) kable(head(df,10)) Stories FinSqft Bedrms Baths LotSize SaleDate SalePrice Ald Age FullBase Attic FIREPLACE AIR_CONDITIONING Garage X Y 1 1560 4 1.5 4680 2012-04 61000 1 76 1 1 0 0 1 2551960 405664.0 1 1538 3 1.5 7100 2012-06 85900 1 65 1 1 1 1 1 2543593 416385.2 1 1359 3 1.0 5040 2012-07 87500 1 72 0 1 0 1 1 2553561 412893.6 1 1339 3 2.5 5000 2012-03 89900 1 70 1 1 0 1 1 2551449 408569.1 1 1213 3 1.0 5125 2012-09 64500 1 61 1 1 0 1 1 2541430 412143.4 1 1206 3 1.0 5880 2012-01 35000 1 72 1 1 0 1 1 2547222 412509.6 1 1062 3 1.5 7560 2012-04 74700 1 71 1 1 0 0 1 2552115 410277.9 1 1033 3 1.0 5320 2012-04 71000 1 67 1 1 0 0 1 2552460 404941.9 2 1664 3 2.5 7800 2012-10 127500 1 63 1 0 1 1 1 2553976 414520.8 2 1237 4 1.5 5542 2012-02 73027 1 42 1 0 0 0 1 2549400 405251.3 cor(x = df$Bedrms,y = df$LotSize,method = &quot;spearman&quot;) ## [1] -0.06929767 # r = -0.06929767 result_7 &lt;- cor.test(x = df$Bedrms,y = df$LotSize) if(result_7$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We cannot reject the null hypothesis.&quot; 6.8 Question 8 This is Exercise 10 in Chapter 7 of the Textbook [R]. 6.8.1 Problem Using R and the Hypothetical UK Housing Prices dataset, find the correlation between floor area and number of bedrooms. 6.8.2 Solution library(knitr) url &lt;- &quot;http://spatial.binghamton.edu/geog533/data/UK_Housing.csv&quot; df &lt;- read.csv(url,header = TRUE) kable(head(df,10)) region price garage bedrooms bathrooms datebuilt floorarea detached fireplace agelessthan15 age65andover nonwhite unemploy ownocc carsperhh manuf 6 25500 0 3 2 1973 70.4 0 0 26.86 8.50 0.038 11.38 52.706 0.583 23.273 6 17450 0 1 1 1982 40.9 0 0 23.22 16.74 1.836 11.73 60.440 0.402 11.524 6 22450 0 2 1 1883 78.6 0 1 20.41 15.79 0.153 5.18 59.692 0.952 12.325 1 22500 0 2 1 1912 81.9 0 0 19.53 15.82 2.881 4.84 59.046 0.745 12.618 5 53500 0 3 1 1959 83.9 0 0 19.69 18.55 0.626 8.08 57.439 0.587 12.593 6 48100 1 2 1 1952 104.8 0 0 18.18 19.69 1.012 0.96 61.262 0.849 7.251 1 29250 0 3 1 1894 136.8 0 0 24.88 17.21 1.653 12.40 59.455 0.178 19.546 1 35450 1 3 1 1975 65.3 0 0 21.82 16.26 1.102 3.46 53.283 0.501 12.353 6 21500 0 2 2 1910 95.6 0 0 21.61 18.59 0.302 9.18 58.230 0.772 16.160 6 28250 0 1 1 1900 76.8 0 0 23.66 15.52 3.506 11.16 51.821 0.277 11.491 cor(df$bedrooms,df$floorarea,method = &quot;spearman&quot;) ## [1] 0.6004428 # r = 0.6004428 result_8 &lt;- cor.test(df$bedrooms,df$floorarea) if(result_8$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We reject the null hypothesis.&quot; 6.9 Question 9 Use the cars data frame in the datasets package to perform the following tasks: 6.9.1 Problem 9a Plot a scatterplot for the data frame (x: speed, y: dist) Solution 9a df &lt;- cars plot(cars) 6.9.2 Problem 9b How many rows in the data frame? Solution 9b n &lt;- nrow(df) The data frame has 50 rows. 6.9.3 Problem 9c Calculate Pearson’s correlation coefficient using the equation below: \\[r=\\frac{\\sum_{n}^{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{(n-1)s_{x}s_{y}}\\] Solution 9c df$speed.z &lt;- scale(df$speed) df$dist.z &lt;- scale(df$dist) df$product &lt;- df$speed.z * df$dist.z r &lt;- sum(df$product) / (n-1) # r = 0.8068949 t &lt;- r*sqrt(n-2)/sqrt(1-r^2) # t = 9.464 t.critical &lt;- qt(0.975,df = n-2) The Pearson’s correlation coefficient is 0.8068949 6.9.4 Problem 9d Use the cor.test() function to find Pearson’s correlation coefficient and compare it to the one from part (c) Solution 9d cor(df$speed,df$dist,method = &quot;pearson&quot;) # r = 0.8068949 ## [1] 0.8068949 result_9d &lt;- cor.test(df$speed,df$dist) # t = 9.464 if(result_9d$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We reject the null hypothesis.&quot; 6.9.5 Problem 9e Calculate Spearman’s rank correlation coefficient using the equation below: \\[r_{S} = 1 - \\frac{6\\sum_{i=1}^{n}d_{i}^{2}}{n^3-n}\\] Solution 9e df$speed.r &lt;- rank(df$speed,ties.method = &quot;average&quot;) df$dist.r &lt;- rank(df$dist,ties.method = &quot;average&quot;) df$d &lt;- (df$speed.r - df$dist.r)^2 r &lt;- 1 - (6*sum(df$d)/(n^3-n)) # r = 0.8308282 The Spearman’s rank correlation coefficient is 0.8308283 6.9.6 Problem 9f Use the cor.test() function to find Spearman’s rank correlation coefficient and compare it to the one from part (e) Solution 9f cor(df$speed,df$dist,method = &quot;spearman&quot;) ## [1] 0.8303568 result_9f &lt;- cor.test(df$speed,df$dist) if(result_9f$p.value&gt;0.05) { print(&quot;We cannot reject the null hypothesis.&quot;) } else { print(&quot;We reject the null hypothesis.&quot;) } ## [1] &quot;We reject the null hypothesis.&quot; "],
["lab-bivariate-regression.html", "7 Lab - Bivariate Regression 7.1 Question 1 7.2 Question 2 7.3 Question 3 7.4 Question 4 7.5 Question 5", " 7 Lab - Bivariate Regression Complete the following exercises in Chapter 8 (Introduction to Regression Analysis) of the textbook [R] pages 247-250. 7.1 Question 1 This is Exercise 1 in Chapter 8 of the Textbook [R]. A regression of weekly shopping trip frequency on annual income (data entered in thousands of dollars) is performed on data collected from 24 respondents. The results are summarized below: Intercept: 0.46 Slope: 0.19 Sum of squares Degrees of freedom Mean square F Regression Residual 1.7 Total 2.3 7.1.1 Question 1(a) Fill in the blanks in the ANOVA table. n &lt;- 24 intercept &lt;- 0.46 slope &lt;- 0.19 TSS &lt;- 2.3 # Total sum of squares RSS &lt;- 1.7 # Residual sum of squares MSS &lt;- TSS - RSS # Model sum of squares df1 &lt;- 1 df2 &lt;- n-2 msq1 &lt;- MSS/df1 # mean square of regression 0.6 msq2 &lt;- RSS/df2 # mean square of residual 0.077 F.value &lt;- msq1/msq2 # 7.76 Sum of squares Degrees of freedom Mean square F Regression 0.6 1 0.6 7.7647059 Residual 1.7 22 0.0772727 Total 2.3 23 7.1.2 Question 1(b) What is the predicted number of weekly shopping trips for someone making $50,000/year? y &lt;- intercept + slope * (50000/1000) # y = 9.96 y ## [1] 9.96 7.1.3 Question 1(c) In words, what is the meaning of the coefficient 0.19? The meaning is that for every increase in annual income equal to $1000, the predicted weekly shopping trip frequency will increase by 0.19. 7.1.4 Question 1(d) Is the regression coefficient significantly different from zero? How do you know? F.critical &lt;- qf(0.95,df1,df2) # 4.30 if(F.value &gt; F.critical) { print(&quot;We reject the null hypothesis&quot;) } else { print(&quot;We fail to reject the null hypothesis&quot;) } ## [1] &quot;We reject the null hypothesis&quot; # since F.value &gt; F.critical, we reject the null hypothesis. In other words, the regression coefficient is significantly different from zero 7.1.5 Question 1(e) What is the value of the correlation coefficient? r.sq &lt;- MSS/TSS # R2 = 0.26 r &lt;- sqrt(r.sq) # r = 0.51 The correlation coefficient is 0.51 7.2 Question 2 This is Exercise 6 in Chapter 8 of the Textbook [R]. The following data are collected in an effort to determine whether snowfall is dependent upon elevation: Snowfall (inches) Elevation (feet) 36 400 78 800 11 200 45 675 Using R, show your work on exercises (a) through (g). 7.2.1 Question 2(a) Find the regression coefficients (the intercept and the slope coefficient). y &lt;- c(36,78,11,45) x &lt;- c(400,800,200,675) n &lt;- length(y) m &lt;- lm(y~x) summary(m) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 ## 4.9772 8.3172 -0.6928 -12.6016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.63721 13.67764 -0.558 0.6328 ## x 0.09665 0.02403 4.022 0.0566 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.25 on 2 degrees of freedom ## Multiple R-squared: 0.8899, Adjusted R-squared: 0.8349 ## F-statistic: 16.17 on 1 and 2 DF, p-value: 0.05663 intercept &lt;- coefficients(m)[1] slope &lt;- coefficients(m)[2] intercept = -7.637206 slope = 0.09665 7.2.2 Question 2(b) Estimate the standard error of the residuals about the regression line. # use the equation on page 236 of the textbook se &lt;- sqrt(sum(residuals(m)^2)/(n-2)) # 11.252 The standard error of the residuals is: 11.2522747 7.2.3 Question 2(c) Test the hypothesis that the regression coefficient associated with the independent variables is equal to zero. Also place a 95% confidence interval on the regression coefficient. summary(m) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 ## 4.9772 8.3172 -0.6928 -12.6016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.63721 13.67764 -0.558 0.6328 ## x 0.09665 0.02403 4.022 0.0566 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.25 on 2 degrees of freedom ## Multiple R-squared: 0.8899, Adjusted R-squared: 0.8349 ## F-statistic: 16.17 on 1 and 2 DF, p-value: 0.05663 anova(m) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 2047.77 2047.77 16.173 0.05663 . ## Residuals 2 253.23 126.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # p-value: 0.05663 # p-value &gt; 0.05, so we fail to reject the null hypothesis that the regression coefficient associated with the independent variables is equal to zero confint(m) ## 2.5 % 97.5 % ## (Intercept) -66.487337152 51.2129252 ## x -0.006754067 0.2000541 # 2.5 % 97.5 % # (Intercept) -66.487337152 51.2129252 # x -0.006754067 0.2000541 7.2.4 Question 2(d) Find the value of \\(r^2\\). summary(m) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 ## 4.9772 8.3172 -0.6928 -12.6016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.63721 13.67764 -0.558 0.6328 ## x 0.09665 0.02403 4.022 0.0566 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.25 on 2 degrees of freedom ## Multiple R-squared: 0.8899, Adjusted R-squared: 0.8349 ## F-statistic: 16.17 on 1 and 2 DF, p-value: 0.05663 # R2 = 0.8899 7.2.5 Question 2(e) Make a table of the observed values, predicted values, and the residuals. df &lt;- data.frame(x,y) df$predict &lt;- fitted(m) df$residual &lt;- residuals(m) library(knitr) kable(df) x y predict residual 400 36 31.02281 4.9771917 800 78 69.68282 8.3171775 200 11 11.69280 -0.6928011 675 45 57.60157 -12.6015681 7.2.6 Question 2(f) Prepare an analysis of variance table portraying the regression results. am &lt;- anova(m) MSS &lt;- am[&quot;Sum Sq&quot;][[1]][1] RSS &lt;- am[&quot;Sum Sq&quot;][[1]][2] TSS &lt;- MSS+RSS df1 &lt;- am[&quot;Df&quot;][[1]][1] df2 &lt;- am[&quot;Df&quot;][[1]][2] df3 &lt;- df1+df2 msq1 &lt;- am[&quot;Mean Sq&quot;][[1]][1] msq2 &lt;- am[&quot;Mean Sq&quot;][[1]][2] F.value &lt;- am[&quot;F value&quot;][[1]][1] Sum of squares Degrees of freedom Mean square F Regression 2047.7726301 1 2047.7726301 16.173391 Residual 253.2273699 2 126.613685 Total 2301 3 7.2.7 Question 2(g) Graph the data and the regression line. plot(x,y) abline(m,col=&quot;red&quot;,lwd=2) 7.3 Question 3 This is Exercise 10 in Chapter 8 of the Textbook [R]. Use R and the Milwaukee dataset to: 7.3.1 Problem perform a regression using sales price as the dependent variable and lot size as the independent variable. 7.3.2 Solution url = &quot;http://spatial.binghamton.edu/geog533/data/Milwaukee_Sales.csv&quot; df &lt;- read.csv(url,header = TRUE) m &lt;- lm(SalePrice ~ LotSize,data = df) summary(m) ## ## Call: ## lm(formula = SalePrice ~ LotSize, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -144724 -42205 -14553 20987 808570 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.298e+04 5.249e+03 17.715 &lt; 2e-16 *** ## LotSize 6.443e+00 7.835e-01 8.223 4.37e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 83090 on 1447 degrees of freedom ## Multiple R-squared: 0.04464, Adjusted R-squared: 0.04398 ## F-statistic: 67.62 on 1 and 1447 DF, p-value: 4.374e-16 plot(df$LotSize,df$SalePrice) abline(m,col=&quot;red&quot;,lwd=2) 7.4 Question 4 This is Exercise 11 in Chapter 8 of the Textbook [R]. Use R and the Hypothetical UK Housing Prices dataset to: 7.4.1 Question 4(a) perform a regression using house price as the dependent variable, and number of bedrooms as the independent variable; url = &quot;http://spatial.binghamton.edu/geog533/data/UK_Housing.csv&quot; df &lt;- read.csv(url,header = TRUE) m &lt;- lm(df$price~df$bedrooms) summary(m) ## ## Call: ## lm(formula = df$price ~ df$bedrooms) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39324 -11511 -3539 8461 107568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11254.1 2312.7 4.866 1.53e-06 *** ## df$bedrooms 11892.6 876.5 13.568 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17800 on 497 degrees of freedom ## Multiple R-squared: 0.2703, Adjusted R-squared: 0.2688 ## F-statistic: 184.1 on 1 and 497 DF, p-value: &lt; 2.2e-16 7.4.2 Question 4(b) repeat part (a), using the number of bathrooms as the independent variable, and comment on the results. m &lt;- lm(df$price~df$bathrooms) summary(m) ## ## Call: ## lm(formula = df$price ~ df$bathrooms) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34955 -15242 -5342 10908 114658 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32729 3149 10.394 &lt; 2e-16 *** ## df$bathrooms 7113 2682 2.653 0.00824 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20690 on 497 degrees of freedom ## Multiple R-squared: 0.01396, Adjusted R-squared: 0.01198 ## F-statistic: 7.036 on 1 and 497 DF, p-value: 0.008245 7.5 Question 5 Download the hometown.csv, append your name and hometown coordinates to the csv, then use leaflet to create a web map (10 pt.) if(!require(leaflet)) install.packages(&quot;leaflet&quot;) library(leaflet) df &lt;- read.csv(&quot;http://spatial.binghamton.edu/leaflet/hometown.csv&quot;) #df &lt;- read.csv(&quot;hometown.csv&quot;) df$latitude = as.character(lapply(strsplit(as.character(df$location), split=&quot;,&quot;), &quot;[&quot;, 1)) df$longitude = as.character(lapply(strsplit(as.character(df$location), split=&quot;,&quot;), &quot;[&quot;, 2)) df$fullname &lt;- paste(df$firstname,df$lastname) df$popup &lt;- paste(&quot;&lt;center&gt;&lt;h4&gt;&quot;,df$fullname,&quot;&lt;/h4&gt;&lt;/center&gt;&quot;,&quot;&lt;img src=&quot;,df$image,&quot; width=&#39;200px&#39;/&gt;&quot;,sep = &quot;&quot;) #write.csv(df,&quot;hometown.csv&quot;,row.names = FALSE) leafIcons &lt;- icons( iconUrl = ifelse(df$type == &quot;grads&quot;, &quot;http://spatial.binghamton.edu/leaflet/images/Blue.png&quot;, &quot;http://spatial.binghamton.edu/leaflet/images/Red.png&quot;)) leaflet(df,width = &quot;100%&quot;,height = 800) %&gt;% addTiles(group = &quot;OSM (default)&quot;) %&gt;% addProviderTiles(provider = &quot;Esri.WorldStreetMap&quot;,group = &quot;World StreetMap&quot;) %&gt;% addProviderTiles(provider = &quot;Esri.WorldImagery&quot;,group = &quot;World Imagery&quot;) %&gt;% addProviderTiles(provider = &quot;NASAGIBS.ViirsEarthAtNight2012&quot;,group = &quot;Nighttime Imagery&quot;) %&gt;% addTiles() %&gt;% fitBounds(147,90,-101,-90) %&gt;% addMarkers(icon =leafIcons,popup = df$popup,clusterOptions = markerClusterOptions()) %&gt;% addLayersControl( baseGroups = c(&quot;OSM (default)&quot;,&quot;World StreetMap&quot;, &quot;World Imagery&quot;, &quot;Nighttime Imagery&quot;), options = layersControlOptions(collapsed = FALSE) ) "],
["lab-multiple-regression.html", "8 Lab - Multiple Regression 8.1 Question 1 8.2 Question 2 8.3 Question 3", " 8 Lab - Multiple Regression Complete the following exercises in Chapter 9 (More on Regression) of the textbook pages 286-287. 8.1 Question 1 This is Exercise 7 in Chapter 9 of the Textbook [R]. The following results were obtained from a regression of \\(n = 14\\) housing prices (in dollars) on median family income, size of house, and size of lot: Sum of squares df Mean square F Regression SS: 4234 3 Residual SS: 3487 Total SS: Coefficient (b) Standard error (sb) VIF Median family income 1.57 0.34 1.3 Size of house (sq.ft) 23.4 11.2 2.9 Size of lot (sq.ft) -9.5 7.1 11.3 Constant 40,000 1000 8.1.1 Question 1(a) Fill in the blanks. 8.1.2 Question 1(b) What is the value of \\(r^2\\)? 8.1.3 Question 1(c) What is the standard error of the estimate? 8.1.4 Question 1(d) Test the null hypothesis that \\(R^2 = 0\\) by comparing the \\(F-statistic\\) from the table with its critical value. 8.1.5 Question 1(e) Are the coefficients in the direction you would hypothesize? If not, which coefficients are opposite in sign from what you would expect? 8.1.6 Question 1(f) Find the \\(t-statistics\\) associated with each coefficient, and test the null hypotheses that the coefficients are equal to zero. Use $ = 0.05$, and be sure to give the critical value of \\(t\\). 8.1.7 Question 1(g) What do you conclude from the variance inflation factors (VIFs)? What (if any) modifications would you recommend in light of the VIFs? 8.1.8 Question 1(h) What is the predicted sales price of a house that is 1500 square feet, on a lot 60´×100´, and in a neighborhood where the median family income is $40,000? 8.2 Question 2 This is Exercise 10 in Chapter 9 of the Textbook [R]. 8.2.1 Question 2(a) Using R and the Hypothetical UK Housing Prices dataset, construct a regression equation using housing price as the dependent variable, and bedrooms, bathrooms, date built, garage, fireplace, floor area, and whether the home is detached as the independent variables. Investigate the importance of multicollinearity and outliers. Comment on the weaknesses of this specification, and on the results. url = &quot;http://spatial.binghamton.edu/geog533/data/UK_Housing.csv&quot; 8.2.2 Question 2(b) Attempt to improve the regression equation found in (a). Justify your decisions in constructing and carrying out the analysis. 8.3 Question 3 This is Exercise 11 in Chapter 9 of the Textbook [R]. 8.3.1 Question 3(a) Using R and the Milwaukee dataset described in Section 1.9.2, construct a regression equation using housing sales price as the dependent variable, and number of bedrooms, lot size, finished square footage in the house, age of house, and number of bathrooms, as the independent variables. Investigate the importance of multicollinearity and outliers. Comment on the weaknesses of this specification, and on the results. url = &quot;http://spatial.binghamton.edu/geog533/data/Milwaukee_Sales.csv&quot; 8.3.2 Question 3(b) Attempt to improve the regression equation found in (a). Justify your decisions in constructing and carrying out the analysis. "],
["midterm-exam.html", "9 Midterm Exam 9.1 Question 1 (25 pt.) 9.2 Question 2 (25 pt.) 9.3 Question 3 (25 pt.) 9.4 Question 4 (25 pt.)", " 9 Midterm Exam What to submit: An R notebook/html document that contains the script/output for each question.File name convention for submissions: lastname_firstname_midterm.zip. You have 60 minutes (8:30-9:30 am) to complete the exam. There will be a 10% penalty per 10 minutes for late submissions. 9.1 Question 1 (25 pt.) The iris data set in the datasets package contains 150 cases (rows) and 5 variables (columns). 9.1.1 Question 1a Randomly select half of the cases as training dataset and the remaining half as test dataset. library(datasets) df &lt;- iris set.seed(100) nrow &lt;- nrow(df) id &lt;- 1:nrow train.index &lt;- sort(sample(id,nrow/2)) train.index ## [1] 9 12 15 20 21 22 24 25 26 27 28 31 32 33 34 37 39 ## [18] 42 45 47 48 51 52 53 55 57 58 59 64 67 69 70 71 72 ## [35] 75 78 81 82 84 85 86 88 89 91 92 95 96 99 102 103 104 ## [52] 109 110 111 112 114 117 118 119 120 121 123 124 125 127 129 133 135 ## [69] 137 138 141 143 146 148 149 test.index &lt;- id[-train.index] test.index ## [1] 1 2 3 4 5 6 7 8 10 11 13 14 16 17 18 19 23 ## [18] 29 30 35 36 38 40 41 43 44 46 49 50 54 56 60 61 62 ## [35] 63 65 66 68 73 74 76 77 79 80 83 87 90 93 94 97 98 ## [52] 100 101 105 106 107 108 113 115 116 122 126 128 130 131 132 134 136 ## [69] 139 140 142 144 145 147 150 train.df &lt;- df[train.index,] test.df &lt;- df[test.index,] if(!(require(DT))) install.packages(&quot;DT&quot;) library(DT) datatable(train.df,caption = &quot;Training dataset&quot;,options = list(pageLength = 5)) datatable(test.df,caption = &quot;Test dataset&quot;,options = list(pageLength = 5)) 9.1.2 Question 1b Find out the number of each species (setosa, versicolor, and virginica) in the training and test datasets, respectively. table(train.df$Species) ## ## setosa versicolor virginica ## 21 27 27 table(test.df$Species) ## ## setosa versicolor virginica ## 29 23 23 9.2 Question 2 (25 pt.) Assume that the probabilities of boy and girl births are 0.505 and 0.495, respectively. 9.2.1 Question 2a What’s the probability of having exactly 50 boys out of 100 births? (X = 50) answer_2a &lt;- dbinom(50,size = 100,prob = 0.505) print(answer_2a) ## [1] 0.07919226 The answer is 0.0791923 9.2.2 Question 2b What’s the probability of having more than 60 boys out of 100 births? (X &gt; 60) answer_2b &lt;- pbinom(60,size = 100,prob = 0.505,lower.tail = FALSE) print(answer_2b) ## [1] 0.02241835 The answer is 0.0224184 9.2.3 Question 2c What’s the probability of having between 40 and 60 boys out of 100 births? (40 &lt; X ≤ 60) answer_2c &lt;- diff(pbinom(c(40,60),size = 100,prob = 0.505)) print(answer_2c) ## [1] 0.9550539 The answer is 0.9550539 9.2.4 Question 2d Plot the density function of the number of boy births out of 100. x &lt;- 0:100 y &lt;- dbinom(x,size = 100,prob = 0.505) plot(x,y,type = &quot;b&quot;,pch=16) 9.3 Question 3 (25 pt.) If cars arrive randomly at a traffic light following posission distribution at the rate of five every ten seconds. 9.3.1 Question 3a What the probability that exactly 30 cars arrive in the next minute (60 seconds)? (X=30) answer_3a &lt;- dpois(30,lambda = 5*6) print(answer_3a) ## [1] 0.07263453 The answer is 0.0726345 9.3.2 Question 3b What’s the probability that more than 40 cars arrive in the next minute (60 seconds)? (X&gt;40) answer_3b &lt;- ppois(40,lambda = 5*6,lower.tail = FALSE) print(answer_3b) ## [1] 0.03230957 The answer is 0.0323096 9.3.3 Question 3c What’s the probability that between 20 and 40 cars arrive in the next minute (60 seconds)? answer_3c &lt;- diff(ppois(c(20,40),lambda = 5*6)) print(answer_3c) ## [1] 0.9324058 The answer is 0.9324058 9.3.4 Question 3d Plot the density function of 0-100 cars arriving in the next minute (60 seconds). x &lt;- 0:100 y &lt;- dpois(x,lambda = 30) plot(x,y,type = &quot;b&quot;,pch=16) 9.4 Question 4 (25 pt.) The number of customers at a bank each day is found to be normally distributed with mean µ = 250 and standard deviation σ=80. 9.4.1 Question 4a What fraction of days will have more than 300 customers? (X&gt;300) answer_4a &lt;- pnorm(300,mean = 250,sd = 80,lower.tail = FALSE) print(answer_4a) ## [1] 0.2659855 The answer is 0.2659855 9.4.2 Question 4b What number of customers will be exceeded 10% of the time? answer_4b &lt;- qnorm(0.1,mean = 250,sd = 80,lower.tail = FALSE) print(answer_4b) ## [1] 352.5241 The answer is 352.5241252 9.4.3 Question 4c What fraction of days will have between 200 and 300 customers? (200 &lt; X≤ 300) answer_4c &lt;- diff(pnorm(c(200,300),mean = 250,sd = 80)) print(answer_4c) ## [1] 0.4680289 The answer is 0.4680289 9.4.4 Question 4d Plot the density function and create the shaded area between 200 and 300 customers. curve(dnorm(x,mean = 250,sd = 80),from = 0,to = 500) cord.x &lt;- c(200,seq(200,300,by = 1),300) cord.y &lt;- c(0,dnorm(seq(200,300,by = 1),mean = 250,sd = 80),0) polygon(cord.x,cord.y,col=&quot;red&quot;) "]
]
